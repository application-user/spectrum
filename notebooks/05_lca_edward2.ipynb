{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we experiment with implementing Latent Credible Analysis models. Let's build the most simpleLCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_probability import edward2 as ed\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.0', '0.9.0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tfp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>object_id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id  object_id  value\n",
       "0          0          0      0\n",
       "1          0          1      1\n",
       "2          1          1      0\n",
       "3          1          0      1\n",
       "4          2          1      2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims = dict()\n",
    "claims['source_id'] = [0, 0, 1, 1, 2]\n",
    "claims['object_id'] = [0, 1, 1, 0, 1]\n",
    "claims['value'] = [0, 1, 0, 1, 2]\n",
    "claims = pd.DataFrame(data=claims)\n",
    "claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectrum.judge.lca_tf import LCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca = LCA()\n",
    "lca.discover(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-8.723231>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_fn = ed.make_log_joint_fn(lca.model)\n",
    "log_prob_fn(claims, **{'z_trusts': [0, 0, 1],\n",
    "                       'z_truth_0': 1,\n",
    "                       'z_truth_1': 1,\n",
    "                       'x_claim_0': 0,\n",
    "                       'x_claim_1': 1,\n",
    "                      'x_claim_2': 0,\n",
    "                      'x_claim_3': 1, \n",
    "                      'x_claim_4': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'honest_probs_q:0' shape=(3,) dtype=float32, numpy=array([0.5, 0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_0_q:0' shape=(2,) dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_1_q:0' shape=(3,) dtype=float32, numpy=array([0.33333334, 0.33333334, 0.33333334], dtype=float32)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lca.mean_field_model(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'honest_probs:0' shape=(3,) dtype=float32, numpy=array([0.5, 0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_0:0' shape=(2,) dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_1:0' shape=(3,) dtype=float32, numpy=array([0.33333334, 0.33333334, 0.33333334], dtype=float32)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lca.model(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ed.tape() as tape:\n",
    "    lca.model(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lca_model(claims):\n",
    "    \"\"\"Build a Latent Credibility Analysis (LCA).\n",
    "   \n",
    "    A LCA model represents a joint distribution p(Y, H, X), where\n",
    "    Y represents hidden truth rvs, H represents data source honesty, and X\n",
    "    represents observation.\n",
    "\n",
    "    Concretely, let's assume that we have M objects, S data sources, and our\n",
    "    observation will be the mask matrix W, `mask`, (see build_mask()), and\n",
    "    observation matrix `observation` (see build_observation()).\n",
    "\n",
    "    With this context, we have:\n",
    "        p(Y, H, X) = product_{m=1,..,M}[p(y_m, H, X)], where\n",
    "        p(y_m, H, X) = p(y_m)product_{s in S_m}[p(b_sm|y_m,s)p(s)],\n",
    "        where S_m are the set of sources that make claims about an object m.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    claims: pd.DataFrame\n",
    "        a data frame that has columns [source_id, object_id, value]\n",
    "\n",
    "    \"\"\"\n",
    "    problem_sizes = claims.nunique()\n",
    "    n_sources = problem_sizes['source_id']\n",
    "    n_objects = problem_sizes['object_id']\n",
    "    domain_size = claims.groupby('object_id').max()['value'] + 1\n",
    "    # create honest rv, H_s, for each sources\n",
    "    honest = []\n",
    "    for s in pyro.plate(name='sources', size=n_sources):\n",
    "        honest.append(\n",
    "            pyro.sample(\n",
    "                f's_{s}',\n",
    "                dist.Categorical(\n",
    "                    probs=pyro.param(f'theta_s_{s}',\n",
    "                                     init_tensor=_draw_probs(),\n",
    "                                     constraint=constraints.simplex))))\n",
    "\n",
    "    # creat hidden truth rv for each object m\n",
    "    hidden_truth = []\n",
    "    for m in pyro.plate(name='objects', size=n_objects):\n",
    "        hidden_truth.append(\n",
    "            pyro.sample(\n",
    "                f'y_{m}',\n",
    "                dist.Categorical(probs=pyro.param(\n",
    "                    f'theta_m_{m}',\n",
    "                    init_tensor=torch.ones((domain_size[m], )) /\n",
    "                    domain_size[m],\n",
    "                    constraint=constraints.simplex))))\n",
    "\n",
    "    for c in pyro.plate(name='claims', size=len(claims.index)):\n",
    "        m = claims.iloc[c]['object_id']\n",
    "        s = claims.iloc[c]['source_id']\n",
    "        y_m = hidden_truth[m]\n",
    "        probs = _build_obj_probs_from_src_honest(pyro.param(f'theta_s_{s}'),\n",
    "                                                 domain_size[m], y_m)\n",
    "        pyro.sample(f'b_{s}_{c}', dist.Categorical(probs=probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model $p(x,z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_desc(claims):\n",
    "    problem_sizes = claims.nunique()\n",
    "    n_sources = problem_sizes['source_id']\n",
    "    n_objects = problem_sizes['object_id']\n",
    "    domain_sizes = claims.groupby('object_id').max()['value'] + 1\n",
    "    return n_sources, n_objects, domain_sizes\n",
    "\n",
    "\n",
    "def model(claims):\n",
    "    \"\"\"a generative model\n",
    "    \n",
    "    We assume each source if it asserts an object's value then it is the one and the only assumption\n",
    "    about that object made by it.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    claims: pd.DataFrame\n",
    "        a data frame that has columns [source_id, object_id, value]\n",
    "        \n",
    "    trainable_variables: list\n",
    "        a list of tf.Variable. These are model parameters.\n",
    "    \"\"\"\n",
    "    n_sources, n_objects, domain_sizes = compute_prob_desc(claims)\n",
    "    \n",
    "    # hidden trusts\n",
    "    honest_probs = tf.Variable(initial_value=tf.ones(n_sources) * 0.5, name='honest_probs')\n",
    "    z_trusts = ed.Bernoulli(name=f'z_trusts', probs=honest_probs)\n",
    "    \n",
    "    # hidden truths\n",
    "    object_probs = []\n",
    "    z_truths = []\n",
    "    for m in domain_sizes.index:\n",
    "        object_probs.append(tf.Variable(initial_value=tf.ones(domain_sizes[m],)/domain_sizes[m], name=f'truth_prob_{m}'))\n",
    "        z_truths.append(ed.Categorical(name=f'z_truth_{m}', probs=object_probs[m]))\n",
    "        \n",
    "    # now generate claims\n",
    "    x_claims = []\n",
    "    for c in claims.index:\n",
    "        s = claims.iloc[c]['source_id']\n",
    "        m = claims.iloc[c]['object_id']\n",
    "        z_truth_m = z_truths[m]  \n",
    "        probs = build_claim_probs(honest_probs[s], domain_sizes[m], z_truth_m.value)\n",
    "        x_claims.append(ed.Categorical(name=f'x_claim_{c}', probs=probs))\n",
    "        \n",
    "    return [honest_probs] + object_probs\n",
    "        \n",
    "def build_claim_probs(honest_prob, domain_size, truth):\n",
    "    mask = tf.reduce_sum(tf.one_hot([truth], domain_size), axis=0)\n",
    "    other = tf.ones(domain_size) - mask\n",
    "    probs = mask*honest_prob*tf.ones(domain_size) + other*((1 - honest_prob) / (domain_size - 1))*tf.ones(domain_size)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational model $p(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_field_model(claims):\n",
    "    \"\"\"a mean field varational model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    claims: pd.DataFrame\n",
    "        a data frame that has columns [source_id, object_id, value]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    trainable_variables: list\n",
    "        a list of tf.Variable. These are variational model parameters.\n",
    "    \"\"\"\n",
    "    n_sources, n_objects, domain_sizes = compute_prob_desc(claims)\n",
    "    \n",
    "    # hidden trust\n",
    "    honest_probs = tf.Variable(initial_value=tf.ones(n_sources) * 0.5, name='honest_probs_q')\n",
    "    z_trusts = ed.Bernoulli(name=f'z_trusts', probs=honest_probs)\n",
    "\n",
    "    # hidden truth\n",
    "    object_probs = []\n",
    "    for m in domain_sizes.index:\n",
    "        object_probs.append(tf.Variable(initial_value=tf.ones(domain_sizes[m],)/domain_sizes[m], name=f'truth_prob_{m}_q'))\n",
    "        ed.Categorical(name=f'z_truth_{m}', probs=object_probs[m])\n",
    "\n",
    "    return [honest_probs] + object_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'honest_probs_q:0' shape=(3,) dtype=float32, numpy=array([0.5, 0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_0_q:0' shape=(2,) dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_1_q:0' shape=(3,) dtype=float32, numpy=array([0.33333334, 0.33333334, 0.33333334], dtype=float32)>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_field_model(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'honest_probs:0' shape=(3,) dtype=float32, numpy=array([0.5, 0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_0:0' shape=(2,) dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>,\n",
       " <tf.Variable 'truth_prob_1:0' shape=(3,) dtype=float32, numpy=array([0.33333334, 0.33333334, 0.33333334], dtype=float32)>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for s in range(10):\n",
    "    with ed.tape() as sample:\n",
    "        mean_field_model(claims)\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-8.723231>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob_fn = ed.make_log_joint_fn(model)\n",
    "log_prob_fn(claims, **{'z_trusts': [0, 0, 1],\n",
    "                       'z_truth_0': 1,\n",
    "                       'z_truth_1': 1,\n",
    "                       'x_claim_0': 0,\n",
    "                       'x_claim_1': 1,\n",
    "                      'x_claim_2': 0,\n",
    "                      'x_claim_3': 1, \n",
    "                      'x_claim_4': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify whether the above computation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_z = tf.math.log(0.5) + tf.math.log(0.5) + tf.math.log(0.5) + tf.math.log(0.5) + tf.math.log(0.33333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_x_object_1 = tf.math.log(0.5) + tf.math.log(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_p_x_object_2 = tf.math.log(0.5) + tf.math.log(0.25) + tf.math.log(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-8.723242>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p_z + log_p_x_object_1 + log_p_x_object_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is our model is correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's very it against our model joint distribution formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
